#!/usr/bin/env python3
"""
üé≠ RESURRECTION PLAN GENERATOR
Automated generation of code optimization and refactoring plans
"""

import os
import json
import re
from datetime import datetime
from collections import defaultdict

class ResurrectionPlanGenerator:
    def __init__(self):
        self.resurrection_plans = {}
        self.optimization_priorities = {
            'critical': 10,
            'high': 7,
            'medium': 4,
            'low': 1
        }
        
        # Optimization templates
        self.optimization_templates = {
            'long_method': {
                'description': 'Break down long method into smaller, focused functions',
                'steps': [
                    'Identify logical sections within the method',
                    'Extract each section into separate functions',
                    'Ensure each function has a single responsibility',
                    'Add proper documentation and type hints'
                ],
                'estimated_time': '2-4 hours',
                'difficulty': 'medium'
            },
            'deep_nesting': {
                'description': 'Reduce nesting levels through early returns and guard clauses',
                'steps': [
                    'Replace nested if statements with early returns',
                    'Extract nested logic into separate functions',
                    'Use guard clauses for input validation',
                    'Consider using polymorphism for complex conditionals'
                ],
                'estimated_time': '1-2 hours',
                'difficulty': 'low'
            },
            'duplicate_code': {
                'description': 'Extract common functionality into reusable functions or classes',
                'steps': [
                    'Identify the duplicated code patterns',
                    'Create a shared function or class method',
                    'Replace duplicated code with calls to the new function',
                    'Ensure consistent behavior across all usages'
                ],
                'estimated_time': '1-3 hours',
                'difficulty': 'medium'
            },
            'magic_numbers': {
                'description': 'Replace magic numbers with named constants',
                'steps': [
                    'Identify all magic numbers in the code',
                    'Create named constants at module or class level',
                    'Replace magic numbers with constant references',
                    'Add documentation explaining the constants'
                ],
                'estimated_time': '30-60 minutes',
                'difficulty': 'low'
            },
            'hardcoded_strings': {
                'description': 'Move hardcoded strings to configuration or constants',
                'steps': [
                    'Identify hardcoded strings that should be configurable',
                    'Create configuration file or constants module',
                    'Replace hardcoded strings with configuration references',
                    'Add validation for configuration values'
                ],
                'estimated_time': '1-2 hours',
                'difficulty': 'low'
            },
            'commented_code': {
                'description': 'Remove or properly handle commented-out code',
                'steps': [
                    'Review commented code for relevance',
                    'Remove obsolete commented code',
                    'Convert useful comments to proper documentation',
                    'Create separate archive for potentially useful commented code'
                ],
                'estimated_time': '30-60 minutes',
                'difficulty': 'low'
            },
            'empty_functions': {
                'description': 'Implement or remove empty function stubs',
                'steps': [
                    'Review empty functions for intended purpose',
                    'Implement functionality or remove if obsolete',
                    'Add proper documentation for implemented functions',
                    'Update any references to removed functions'
                ],
                'estimated_time': '1-2 hours',
                'difficulty': 'medium'
            },
            'debug_prints': {
                'description': 'Replace debug prints with proper logging',
                'steps': [
                    'Set up proper logging configuration',
                    'Replace print statements with appropriate log levels',
                    'Add log formatting and output configuration',
                    'Remove or conditionalize debug logging in production'
                ],
                'estimated_time': '1-2 hours',
                'difficulty': 'low'
            },
            'missing_docs': {
                'description': 'Add comprehensive documentation to functions',
                'steps': [
                    'Write docstrings for all undocumented functions',
                    'Include parameter descriptions and return value documentation',
                    'Add usage examples where appropriate',
                    'Ensure consistent documentation format'
                ],
                'estimated_time': '2-4 hours',
                'difficulty': 'medium'
            },
            'wildcard_imports': {
                'description': 'Replace wildcard imports with explicit imports',
                'steps': [
                    'Identify all wildcard import statements',
                    'Determine which specific items are actually used',
                    'Replace wildcard imports with explicit imports',
                    'Remove unused imports'
                ],
                'estimated_time': '30-60 minutes',
                'difficulty': 'low'
            }
        }

    def load_pattern_results(self):
        """Load pattern detection results"""
        try:
            with open('necromancy_graveyard/pattern_detection_results.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print("‚ùå Pattern detection results not found. Run necromancy_pattern_detector.py first.")
            return {}
        except json.JSONDecodeError:
            print("‚ùå Error reading pattern detection results.")
            return {}

    def calculate_optimization_score(self, patterns):
        """Calculate optimization score for a file"""
        score = 0
        for category, category_patterns in patterns.items():
            for pattern_name, pattern_data in category_patterns.items():
                severity = pattern_data.get('severity', 'low')
                count = pattern_data.get('count', 1)
                score += self.optimization_priorities.get(severity, 1) * count
        
        return score

    def generate_file_resurrection_plan(self, filepath, patterns):
        """Generate resurrection plan for a specific file"""
        plan = {
            'file': filepath,
            'optimization_score': self.calculate_optimization_score(patterns),
            'patterns_found': len(sum([list(v.keys()) for v in patterns.values()], [])),
            'optimizations': [],
            'estimated_total_time': '0 hours',
            'difficulty_rating': 'unknown',
            'priority_level': 'low'
        }
        
        total_time_minutes = 0
        max_difficulty_score = 0
        
        for category, category_patterns in patterns.items():
            for pattern_name, pattern_data in category_patterns.items():
                if pattern_name in self.optimization_templates:
                    template = self.optimization_templates[pattern_name]
                    
                    optimization = {
                        'pattern': pattern_name,
                        'severity': pattern_data.get('severity', 'low'),
                        'description': template['description'],
                        'steps': template['steps'],
                        'estimated_time': template['estimated_time'],
                        'difficulty': template['difficulty']
                    }
                    
                    # Add pattern-specific details
                    if 'count' in pattern_data:
                        optimization['count'] = pattern_data['count']
                    if 'examples' in pattern_data:
                        optimization['examples'] = pattern_data['examples']
                    
                    plan['optimizations'].append(optimization)
                    
                    # Calculate time
                    time_match = re.match(r'(\d+)-(\d+)\s*(hours?|minutes?)', template['estimated_time'])
                    if time_match:
                        min_time = int(time_match.group(1))
                        max_time = int(time_match.group(2))
                        unit = time_match.group(3)
                        
                        avg_time = (min_time + max_time) / 2
                        if unit.startswith('hour'):
                            total_time_minutes += avg_time * 60
                        else:
                            total_time_minutes += avg_time
                    
                    # Track difficulty
                    difficulty_scores = {'low': 1, 'medium': 2, 'high': 3}
                    max_difficulty_score = max(max_difficulty_score, difficulty_scores.get(template['difficulty'], 1))
        
        # Calculate total time
        if total_time_minutes < 60:
            plan['estimated_total_time'] = f"{int(total_time_minutes)} minutes"
        else:
            hours = int(total_time_minutes // 60)
            minutes = int(total_time_minutes % 60)
            plan['estimated_total_time'] = f"{hours} hours {minutes} minutes"
        
        # Set difficulty rating
        difficulty_ratings = {1: 'low', 2: 'medium', 3: 'high'}
        plan['difficulty_rating'] = difficulty_ratings.get(max_difficulty_score, 'unknown')
        
        # Set priority level
        if plan['optimization_score'] >= 20:
            plan['priority_level'] = 'critical'
        elif plan['optimization_score'] >= 10:
            plan['priority_level'] = 'high'
        elif plan['optimization_score'] >= 5:
            plan['priority_level'] = 'medium'
        else:
            plan['priority_level'] = 'low'
        
        return plan

    def generate_repository_resurrection_plan(self):
        """Generate comprehensive resurrection plan for entire repository"""
        print("üîÑ GENERATING REPOSITORY RESURRECTION PLAN...")
        
        pattern_results = self.load_pattern_results()
        if not pattern_results:
            return None
        
        resurrection_plan = {
            'generated_at': datetime.now().isoformat(),
            'repository_overview': {
                'total_files': len(pattern_results),
                'total_patterns': sum(sum(len(v) for v in patterns.values()) for patterns in pattern_results.values()),
                'average_patterns_per_file': 0,
                'optimization_priority_distribution': defaultdict(int)
            },
            'file_plans': [],
            'global_optimizations': [],
            'implementation_phases': [],
            'estimated_timeline': {},
            'success_metrics': []
        }
        
        if resurrection_plan['repository_overview']['total_files'] > 0:
            resurrection_plan['repository_overview']['average_patterns_per_file'] = round(
                resurrection_plan['repository_overview']['total_patterns'] / resurrection_plan['repository_overview']['total_files'], 2
            )
        
        # Generate individual file plans
        for filepath, patterns in pattern_results.items():
            file_plan = self.generate_file_resurrection_plan(filepath, patterns)
            resurrection_plan['file_plans'].append(file_plan)
            resurrection_plan['repository_overview']['optimization_priority_distribution'][file_plan['priority_level']] += 1
        
        # Sort file plans by priority and optimization score
        resurrection_plan['file_plans'].sort(key=lambda x: (
            {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}[x['priority_level']],
            x['optimization_score']
        ), reverse=True)
        
        # Identify global optimization patterns
        global_patterns = self.identify_global_patterns(pattern_results)
        resurrection_plan['global_optimizations'] = global_patterns
        
        # Create implementation phases
        resurrection_plan['implementation_phases'] = self.create_implementation_phases(resurrection_plan['file_plans'])
        
        # Estimate timeline
        resurrection_plan['estimated_timeline'] = self.estimate_timeline(resurrection_plan['file_plans'])
        
        # Define success metrics
        resurrection_plan['success_metrics'] = self.define_success_metrics(resurrection_plan)
        
        return resurrection_plan

    def identify_global_patterns(self, pattern_results):
        """Identify patterns that appear across multiple files"""
        global_patterns = defaultdict(lambda: {'files': [], 'total_count': 0, 'severity_distribution': defaultdict(int)})
        
        for filepath, patterns in pattern_results.items():
            for category, category_patterns in patterns.items():
                for pattern_name, pattern_data in category_patterns.items():
                    global_patterns[pattern_name]['files'].append(filepath)
                    global_patterns[pattern_name]['total_count'] += pattern_data.get('count', 1)
                    global_patterns[pattern_name]['severity_distribution'][pattern_data.get('severity', 'low')] += 1
        
        # Convert to list and sort by impact
        global_list = []
        for pattern_name, data in global_patterns.items():
            if len(data['files']) > 1:  # Only include patterns that appear in multiple files
                global_list.append({
                    'pattern': pattern_name,
                    'affected_files': len(data['files']),
                    'total_occurrences': data['total_count'],
                    'severity_breakdown': dict(data['severity_distribution']),
                    'files': data['files'][:5]  # Limit to first 5 files
                })
        
        global_list.sort(key=lambda x: (x['affected_files'], x['total_occurrences']), reverse=True)
        return global_list

    def create_implementation_phases(self, file_plans):
        """Create phased implementation plan"""
        phases = [
            {
                'phase': 'Phase 1: Critical Fixes',
                'description': 'Address critical issues that impact functionality and security',
                'files': [p['file'] for p in file_plans if p['priority_level'] == 'critical'],
                'estimated_time': '1-2 days',
                'focus': 'Fix critical bugs and security issues'
            },
            {
                'phase': 'Phase 2: High Impact Optimizations',
                'description': 'Implement high-impact optimizations for performance and maintainability',
                'files': [p['file'] for p in file_plans if p['priority_level'] == 'high'],
                'estimated_time': '3-5 days',
                'focus': 'Performance improvements and code quality'
            },
            {
                'phase': 'Phase 3: Medium Priority Refactoring',
                'description': 'Address medium-priority issues for better code organization',
                'files': [p['file'] for p in file_plans if p['priority_level'] == 'medium'],
                'estimated_time': '1-2 weeks',
                'focus': 'Code organization and documentation'
            },
            {
                'phase': 'Phase 4: Low Priority Cleanup',
                'description': 'Clean up low-priority issues and polish the codebase',
                'files': [p['file'] for p in file_plans if p['priority_level'] == 'low'],
                'estimated_time': '1-2 weeks',
                'focus': 'Code cleanup and final polishing'
            }
        ]
        
        # Remove empty phases
        phases = [phase for phase in phases if phase['files']]
        
        return phases

    def estimate_timeline(self, file_plans):
        """Estimate overall timeline for resurrection plan"""
        total_time_minutes = 0
        
        for plan in file_plans:
            time_str = plan['estimated_total_time']
            if 'minutes' in time_str:
                total_time_minutes += int(time_str.split()[0])
            elif 'hours' in time_str:
                parts = time_str.split()
                hours = int(parts[0])
                if len(parts) > 2:
                    minutes = int(parts[2])
                else:
                    minutes = 0
                total_time_minutes += hours * 60 + minutes
        
        # Convert to days and weeks
        total_hours = total_time_minutes / 60
        total_days = total_hours / 8  # Assuming 8-hour workdays
        
        if total_days <= 1:
            timeline = f"{total_hours:.1f} hours"
        elif total_days <= 7:
            timeline = f"{total_days:.1f} days"
        else:
            weeks = total_days / 5  # Assuming 5-day work weeks
            timeline = f"{weeks:.1f} weeks"
        
        return {
            'total_estimated_time': timeline,
            'total_files_to_optimize': len(file_plans),
            'average_time_per_file': f"{total_time_minutes / max(len(file_plans), 1):.0f} minutes",
            'recommended_schedule': '2-3 hours per day to maintain code quality'
        }

    def define_success_metrics(self, resurrection_plan):
        """Define success metrics for the resurrection plan"""
        return [
            {
                'metric': 'Code Quality Improvement',
                'target': f"Reduce total patterns by {resurrection_plan['repository_overview']['total_patterns'] * 0.7:.0f} ({resurrection_plan['repository_overview']['total_patterns']})",
                'measurement': 'Pattern count reduction'
            },
            {
                'metric': 'Performance Enhancement',
                'target': 'Improve execution time by 20-30%',
                'measurement': 'Benchmark comparisons before/after optimization'
            },
            {
                'metric': 'Maintainability Score',
                'target': 'Increase maintainability index by 25 points',
                'measurement': 'Code analysis tools (e.g., radon, pylint)'
            },
            {
                'metric': 'Documentation Coverage',
                'target': 'Achieve 90% documentation coverage',
                'measurement': 'Documentation analysis tools'
            },
            {
                'metric': 'Error Reduction',
                'target': 'Reduce runtime errors by 50%',
                'measurement': 'Error logging and monitoring'
            }
        ]

    def generate_resurrection_report(self, resurrection_plan):
        """Generate comprehensive resurrection report"""
        if not resurrection_plan:
            return "‚ùå Unable to generate resurrection plan. Run pattern detection first."
        
        report = f"""# ‚ö∞Ô∏è CODE RESURRECTION PLAN
# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä REPOSITORY OVERVIEW
- **Files Analyzed:** {resurrection_plan['repository_overview']['total_files']}
- **Total Patterns Found:** {resurrection_plan['repository_overview']['total_patterns']}
- **Average Patterns per File:** {resurrection_plan['repository_overview']['average_patterns_per_file']}
- **Estimated Total Time:** {resurrection_plan['estimated_timeline']['total_estimated_time']}

## üéØ OPTIMIZATION PRIORITY DISTRIBUTION
"""
        
        for priority, count in resurrection_plan['repository_overview']['optimization_priority_distribution'].items():
            report += f"- **{priority.title()}:** {count} files\n"
        
        report += "\n## üìÅ TOP PRIORITY FILES\n\n"
        
        for i, file_plan in enumerate(resurrection_plan['file_plans'][:10], 1):
            report += f"### {i}. {os.path.basename(file_plan['file'])}\n"
            report += f"- **Priority:** {file_plan['priority_level'].title()}\n"
            report += f"- **Optimization Score:** {file_plan['optimization_score']}\n"
            report += f"- **Patterns Found:** {file_plan['patterns_found']}\n"
            report += f"- **Estimated Time:** {file_plan['estimated_total_time']}\n"
            report += f"- **Difficulty:** {file_plan['difficulty_rating'].title()}\n\n"
            
            if file_plan['optimizations']:
                report += "**Key Optimizations:**\n"
                for opt in file_plan['optimizations'][:3]:
                    report += f"- {opt['pattern'].replace('_', ' ').title()}: {opt['description']}\n"
                report += "\n"
        
        if resurrection_plan['global_optimizations']:
            report += "## üåç GLOBAL OPTIMIZATION PATTERNS\n\n"
            for i, pattern in enumerate(resurrection_plan['global_optimizations'][:5], 1):
                report += f"### {i}. {pattern['pattern'].replace('_', ' ').title()}\n"
                report += f"- **Affected Files:** {pattern['affected_files']}\n"
                report += f"- **Total Occurrences:** {pattern['total_occurrences']}\n"
                report += f"- **Severity Breakdown:** {', '.join(f'{k}: {v}' for k, v in pattern['severity_breakdown'].items())}\n\n"
        
        report += "## üìÖ IMPLEMENTATION PHASES\n\n"
        for phase in resurrection_plan['implementation_phases']:
            report += f"### {phase['phase']}\n"
            report += f"**Description:** {phase['description']}\n"
            report += f"**Files:** {len(phase['files'])}\n"
            report += f"**Estimated Time:** {phase['estimated_time']}\n"
            report += f"**Focus:** {phase['focus']}\n\n"
        
        report += "## üìà SUCCESS METRICS\n\n"
        for metric in resurrection_plan['success_metrics']:
            report += f"### {metric['metric']}\n"
            report += f"- **Target:** {metric['target']}\n"
            report += f"- **Measurement:** {metric['measurement']}\n\n"
        
        report += """## üöÄ EXECUTION GUIDELINES

### Phase 1: Preparation (1-2 hours)
- [ ] Review this resurrection plan thoroughly
- [ ] Set up development environment and testing framework
- [ ] Create backup branch for safety
- [ ] Establish baseline metrics (performance, code quality)

### Phase 2: Critical Fixes (1-2 days)
- [ ] Start with critical priority files
- [ ] Implement fixes with comprehensive testing
- [ ] Commit changes frequently with descriptive messages
- [ ] Update documentation as you go

### Phase 3: Systematic Optimization (1-2 weeks)
- [ ] Follow the phased approach outlined above
- [ ] Maintain code quality standards throughout
- [ ] Regular code reviews and testing
- [ ] Monitor progress against success metrics

### Phase 4: Polish and Validate (2-3 days)
- [ ] Final cleanup of low-priority issues
- [ ] Comprehensive testing across the codebase
- [ ] Performance benchmarking
- [ ] Documentation updates

## üõ†Ô∏è TOOLS AND TECHNIQUES

### Code Quality Tools
- **Linters:** pylint, flake8, black
- **Type Checkers:** mypy
- **Complexity Analyzers:** radon
- **Documentation:** sphinx, pydoc

### Testing Framework
- **Unit Tests:** pytest
- **Integration Tests:** pytest with fixtures
- **Performance Tests:** pytest-benchmark
- **Coverage:** pytest-cov

### Development Practices
- **Version Control:** Git with feature branches
- **Code Reviews:** Pull requests with automated checks
- **CI/CD:** GitHub Actions for automated testing
- **Documentation:** Auto-generated API docs

---

*This resurrection plan provides a systematic approach to code optimization and quality improvement.*
"""
        
        return report

    def save_resurrection_plan(self, resurrection_plan):
        """Save resurrection plan to files"""
        os.makedirs('necromancy_graveyard', exist_ok=True)
        
        # Save detailed plan
        with open('necromancy_graveyard/resurrection_plan.json', 'w', encoding='utf-8') as f:
            json.dump(resurrection_plan, f, indent=2, ensure_ascii=False)
        
        # Save human-readable report
        report = self.generate_resurrection_report(resurrection_plan)
        with open('necromancy_graveyard/RESURRECTION_PLAN.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("‚úÖ Resurrection plan generated and saved!")

    def run_resurrection_planning(self):
        """Run complete resurrection planning process"""
        print("‚ö∞Ô∏è INITIATING RESURRECTION PLAN GENERATION...")
        
        resurrection_plan = self.generate_repository_resurrection_plan()
        
        if resurrection_plan:
            self.save_resurrection_plan(resurrection_plan)
            
            print("\n" + "="*60)
            print("üé≠ RESURRECTION PLAN COMPLETE")
            print("="*60)
            print(f"üìÅ Files to Optimize: {resurrection_plan['repository_overview']['total_files']}")
            print(f"üéØ Total Patterns: {resurrection_plan['repository_overview']['total_patterns']}")
            print(f"‚è±Ô∏è  Estimated Time: {resurrection_plan['estimated_timeline']['total_estimated_time']}")
            print(f"üìã Plan: necromancy_graveyard/RESURRECTION_PLAN.md")
            print("="*60)
            print("Ready for systematic code optimization!")
            
            return resurrection_plan
        else:
            print("‚ùå Failed to generate resurrection plan.")
            return None

def main():
    generator = ResurrectionPlanGenerator()
    plan = generator.run_resurrection_planning()
    return plan

if __name__ == "__main__":
    main()
